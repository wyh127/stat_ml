zip.3<-read.table("train.3.txt", header=FALSE, sep=",")
zip.3<-as.matrix(zip.3)
zip.5<-read.table("train.5.txt", header=FALSE, sep=",")
zip.5<-as.matrix(zip.5)
### n.3 and n.5 are the total number of "3"s and "5"s, respectively.
n.3<-length(zip.3[,1])
n.5<-length(zip.5[,1])
### combine two data sets together
data<-rbind(zip.3, zip.5)
print(zip.3)
print(head(zip.3))
print(dim(zip.3))
print(dim(zip.5))
n.3
print(dim(data))
output.image<-function(vector) {
digit<-matrix(vector, nrow=16, ncol=16)
#index= seq(from=1, to =16, by=1)
index= seq(from=16, to =1, by=-1)
sym_digit = digit[,index]
image(sym_digit, col= gray((8:0)/8), axes=FALSE)
}
par(mfrow=c(1,1),mai=c(0.6,0.6,0.6,0.6))
mean.3<- apply(zip.3, 2, mean)
### visualize the mean ###
output.image(mean.3)
output.image<-function(vector) {
digit<-matrix(vector, nrow=16, ncol=16)
#index= seq(from=1, to =16, by=1)
index= seq(from=16, to =1, by=-1)
sym_digit = digit[,]
image(sym_digit, col= gray((8:0)/8), axes=FALSE)
}
par(mfrow=c(1,1),mai=c(0.6,0.6,0.6,0.6))
mean.3<- apply(zip.3, 2, mean)
### visualize the mean ###
output.image(mean.3)
scaled.3<-scale(zip.3,center=TRUE, scale=FALSE)
pca<-svd(scaled.3)
par(mfrow=c(4,4), mai=c(0.1,0.1, 0.1, 0.1))
for(j in 1:16) {
output.image(pca$v[,j])
}
pca<-svd(scaled.3)
par(mfrow=c(4,4), mai=c(0.1,0.1, 0.1, 0.1))
for(j in 1:17) {
output.image(pca$v[,j])
}
pca<-svd(scaled.3)
par(mfrow=c(4,4), mai=c(0.1,0.1, 0.1, 0.1))
for(j in 1:16) {
output.image(pca$v[,j])
}
pca<-svd(scaled.3)
par(mfrow=c(4,4), mai=c(0.1,0.1, 0.1, 0.1))
for(j in 1:256) {
output.image(pca$v[,j])
}
pca<-svd(scaled.3)
par(mfrow=c(4,4), mai=c(0.1,0.1, 0.1, 0.1))
for(j in 1:16) {
output.image(pca$v[,j])
}
scaled.data<-scale(data, center=TRUE, scale=FALSE)
pca<-svd(scaled.data)
par(mfrow=c(1,1), mai=c(0.6, 0.6, 0.6, 0.6))
plot(pca$d[1]* pca$u[,1], pca$d[2]* pca$u[, 2],pch=16, xlab="First Principle Component", ylab="Second Principle Component" )
pca$d[1]* pca$u[,1]
pca$d[1]
plot(pca$d[1]*pca$u[1:n.3, 1], pca$d[2]*pca$u[1:n.3, 2], pch="3", col="red", cex=0.8,xlim=c(-10, 10), ylim=c(-10, 10), xlab="First Principle Component", ylab="Second Principle Component")
points(pca$d[1]*pca$u[(n.3+1):(n.3+n.5), 1], pca$d[2]* pca$u[(n.3+1):(n.3+n.5), 2], cex=0.8,pch="5", col="blue")
output.image<-function(vector) {
digit<-matrix(vector, nrow=16, ncol=16)
#index= seq(from=1, to =16, by=1)
index= seq(from=16, to =1, by=-1)
# sym_digit = digit[, index]
image(digit, col= gray((8:0)/8), axes=FALSE)
}
par(mfrow=c(10,10),mai=c(0.1,0.1,0.1,0.1))
#par(mfrow=c(1,1),mai=c(0.1,0.1,0.1,0.1))
for(i in 1:100) {
#output.image(zip.5[i,])
output.image(zip.3[i,])
}
output.image<-function(vector) {
digit<-matrix(vector, nrow=16, ncol=16)
#index= seq(from=1, to =16, by=1)
index= seq(from=16, to =1, by=-1)
sym_digit = digit[, index]
image(sym_digit, col= gray((8:0)/8), axes=FALSE)
}
par(mfrow=c(10,10),mai=c(0.1,0.1,0.1,0.1))
#par(mfrow=c(1,1),mai=c(0.1,0.1,0.1,0.1))
for(i in 1:100) {
#output.image(zip.5[i,])
output.image(zip.3[i,])
}
plot(seq(from=1,to=256, by=1), (pca$d)^2/sum((pca$d)^2), xlab="Priciple componnets", ylab="Proportion of variance explained", pch=16)
km.out<- kmeans(data, 2, nstart=50)
digit_centers<-km.out$centers
par(mfrow=c(1,2), mai=c(0.1,0.1,0.1,0.1))
for(i in 1:2) {
output.image(digit_centers[i,])
}
output.image<-function(vector) {
digit<-matrix(vector, nrow=16, ncol=16)
#index= seq(from=1, to =16, by=1)
index= seq(from=16, to =1, by=-1)
sym_digit = digit[, index]
image(sym_digit, col= gray((8:0)/8), axes=FALSE)
}
par(mfrow=c(10,10),mai=c(0.1,0.1,0.1,0.1))
#par(mfrow=c(1,1),mai=c(0.1,0.1,0.1,0.1))
for(i in 1:100) {
#output.image(zip.5[i,])
output.image(zip.3[i,])
}
output.image<-function(vector) {
digit<-matrix(vector, nrow=16, ncol=16)
#index= seq(from=1, to =16, by=1)
#index= seq(from=16, to =1, by=-1)
#sym_digit = digit[, index]
image(digit, col= gray((8:0)/8), axes=FALSE)
}
par(mfrow=c(10,10),mai=c(0.1,0.1,0.1,0.1))
#par(mfrow=c(1,1),mai=c(0.1,0.1,0.1,0.1))
for(i in 1:100) {
#output.image(zip.5[i,])
output.image(zip.3[i,])
}
km.out<- kmeans(data, 5, nstart=50)
digit_centers<-km.out$centers
par(mfrow=c(1,2), mai=c(0.1,0.1,0.1,0.1))
for(i in 1:2) {
output.image(digit_centers[i,])
}
km.out<- kmeans(data, 2, nstart=50)
km.out<- kmeans(data, 2, nstart=50)
digit_centers<-km.out$centers
par(mfrow=c(1,2), mai=c(0.1,0.1,0.1,0.1))
for(i in 1:2) {
output.image(digit_centers[i,])
}
library(datasets)
statedata=as.data.frame(state.x77)
colnames(statedata)=c("popu", "inc", "illit", "life.exp", "murder", "hs.grad", "frost", "area")
plot(life.exp~inc, data=statedata)
cor(statedata[,"life.exp"], statedata[,"inc"])
plot(life.exp~inc, data=statedata, type="n")
text(life.exp~inc, data=statedata, state.abb)
model1=lm(life.exp~inc, data=statedata)
summary(model1)
plot(life.exp~inc, data=statedata,
xlab="Life Expectancy", ylab="Income")
abline(model1)
par(mfrow=c(2,2)) # create a panel of four plotting areas
for(i in 1:4){
## Plot the population
plot(life.exp~inc, data=statedata,
xlab="Life Expectancy", ylab="Income",
title=paste("Random sample", format(i)),
ylim=c(min(life.exp), max(life.exp)+0.3))
abline(model1)
if(i==1){
legend(3030, 74.2,
pch=c(NA, NA, NA, 1, 16),
lty=c(1, 1, 2, NA, NA),
col=c(1, 2, 2, 1, 2),
c("population truth", "sample estimate",
"sample confidence band",
"states", "sampled"),
cex=0.7,
bty="n"
)
}
## Select the sample
selected.states=sample(1:50, 10)
points(statedata[selected.states,"inc"],
statedata[selected.states,"life.exp"], pch=16, col=2)
## Fit a regression line using the sample
model.sel = lm(life.exp~inc, data=statedata[selected.states,])
abline(model.sel, col=2)
## Make a confidence band.
#### first calculate the width of the band, W.
ww=qt(0.975, 10-2)
#### generate plotting X values.
plot.x<-data.frame(inc=seq(3000, 7000, 1))
#### se.fit=T is an option to save
#### the standard error of the fitted values.
plot.fit<-predict(model.sel, plot.x,
level=0.95, interval="confidence",
se.fit=T)
#### lines is a function to add connected lines
#### to an existing plot.
lines(plot.x$inc, plot.fit$fit[,1]+ww*plot.fit$se.fit,
col=2, lty=2)
lines(plot.x$inc, plot.fit$fit[,1]-ww*plot.fit$se.fit,
col=2, lty=2)
}
for(i in 1:4){
## Plot the population
plot(life.exp~inc, data=statedata,
xlab="Life Expectancy", ylab="Income",
title=paste("Random sample", format(i)),
ylim=c(min(life.exp), max(life.exp)+0.3))
abline(model1)
if(i==1){
legend(3030, 74.2,
pch=c(NA, NA, NA, 1, 16),
lty=c(1, 1, 2, NA, NA),
col=c(1, 2, 2, 1, 2),
c("population truth", "sample estimate",
"sample confidence band",
"states", "sampled"),
cex=0.7,
bty="n"
)
}
## Select the sample
selected.states=sample(1:50, 10)
points(statedata[selected.states,"inc"],
statedata[selected.states,"life.exp"], pch=16, col=2)
## Fit a regression line using the sample
model.sel = lm(life.exp~inc, data=statedata[selected.states,])
abline(model.sel, col=2)
## Make a confidence band.
#### first calculate the width of the band, W.
ww=qt(0.975, 10-2)
#### generate plotting X values.
plot.x<-data.frame(inc=seq(3000, 7000, 1))
#### se.fit=T is an option to save
#### the standard error of the fitted values.
plot.fit<-predict(model.sel, plot.x,
level=0.95, interval="confidence",
se.fit=T)
#### lines is a function to add connected lines
#### to an existing plot.
lines(plot.x$inc, plot.fit$fit[,1]+ww*plot.fit$se.fit,
col=2, lty=2)
lines(plot.x$inc, plot.fit$fit[,1]-ww*plot.fit$se.fit,
col=2, lty=2)
}
library(MASS)
library(ISLR)
attach(Boston)
lm.fit=lm(medv ~ lstat+age,data=Boston)
summary(lm.fit)
lm.fit=lm(medv~.,data=Boston)
summary(lm.fit)
lm.fit1=lm(medv ~.-age,data=Boston)
summary(lm.fit1)
summary(lm(medv ~ lstat*age,data=Boston))
plot(lstat, medv, pch=16)
lm.fit2=lm(medv ~ lstat+I(lstat^2))
summary(lm.fit2)
lm.fit2=lm(medv ~ lstat+lstat^2)
summary(lm.fit2)
lm.fit=lm(medv~lstat)
anova(lm.fit ,lm.fit2)
lm.fit=lm(medv~lstat)
anova(lm.fit ,lm.fit2)
lm.fit=lm(medv~lstat)
anova(lm.fit ,lm.fit2)
lm.fit2=lm(medv ~ lstat+I(lstat^2))
summary(lm.fit2)
lm.fit=lm(medv~lstat)
anova(lm.fit ,lm.fit2)
par(mfrow=c(2,2))
plot(lm.fit2)
names(Carseats )
View(Carseats)
lm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)
summary(lm.fit)
attach(Carseats)
contrasts(ShelveLoc)
iris = read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data", sep = ",", header = FALSE)
names(iris) = c("sepal.length", "sepal.width", "petal.length", "petal.width", "iris.type")
### attach name to each column so that we can directly access each column by its name
attach(iris)
train = sample.int(nrow(iris), 100)
Y = iris.type == "Iris-setosa"
logistic.model = glm(Y ~ sepal.length + sepal.width, data=iris, family = binomial(), subset=train)
logistic.model
plot(sepal.length[train], sepal.width[train], type='p',pch=16, col=(Y[train]+4), xlab="Sepal Length", ylab="Sepal Width")
abline(a = -logistic.model$coefficients[1]/logistic.model$coefficients[3], b = -logistic.model$coefficients[2]/logistic.model$coefficients[3], col='gray', lwd=2)
glm.probs = predict(logistic.model, iris[-train,], type="response")
glm.pred = glm.probs>0.5
### summrize the prediction by a confusion matrix
table(Y[-train], glm.pred)
library(MASS)
lda.model<-lda(Y ~ sepal.length + sepal.width, data=iris, subset=train)
lda.model
plot(lda.model)
lda.pred = predict(lda.model, iris[-train,])
table(Y[-train], lda.pred$class)
